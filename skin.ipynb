{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cc7e43c-1b70-402c-86fd-585e27c109e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10015 files were not found and could not be moved.\n",
      "Images successfully organized into class folders!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Load metadata\n",
    "df = pd.read_csv('Downloads/data/HAM10000_metadata.csv')\n",
    "\n",
    "# Define directories\n",
    "image_folder = 'Downloads/data/images/'\n",
    "train_folder = 'Downloads/data/train'\n",
    "\n",
    "# Define class labels\n",
    "CLASS_LABELS = df['dx'].unique().tolist()\n",
    "\n",
    "# Create folders for each class\n",
    "for label in CLASS_LABELS:\n",
    "    os.makedirs(os.path.join(train_folder, label), exist_ok=True)\n",
    "\n",
    "# Move images into corresponding class folders\n",
    "missing_files = []\n",
    "for _, row in df.iterrows():\n",
    "    filename = row['image_id'] + '.jpg'\n",
    "    src = os.path.join(image_folder, filename)\n",
    "    dst = os.path.join(train_folder, row['dx'], filename)\n",
    "    \n",
    "    # Check if the image file exists\n",
    "    if os.path.exists(src):\n",
    "        shutil.move(src, dst)\n",
    "    else:\n",
    "        missing_files.append(filename)\n",
    "\n",
    "# Report missing files\n",
    "if missing_files:\n",
    "    print(f\"{len(missing_files)} files were not found and could not be moved.\")\n",
    "    #print(\"List of missing files:\")\n",
    "    #for file in missing_files:\n",
    "        #print(file)\n",
    "\n",
    "print(\"Images successfully organized into class folders!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3973bc-1c32-495d-a802-173018a24e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "# Create validation directories\n",
    "for label in CLASS_LABELS:\n",
    "    os.makedirs(f'Downloads/data/val/{label}', exist_ok=True)\n",
    "\n",
    "# Split images into train/val\n",
    "for label in CLASS_LABELS:\n",
    "    image_files = os.listdir(f\"Downloads/data/train/{label}\")\n",
    "    train_files, val_files = train_test_split(image_files, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Move validation images\n",
    "    for file in val_files:\n",
    "        shutil.move(f\"Downloads/data/train/{label}/{file}\", f\"Downloads/data/val/{label}/{file}\")\n",
    "\n",
    "print(\"Train/Validation split completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7eec2ab-e4bb-40c1-8ebe-5239b608259c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8010 images belonging to 7 classes.\n",
      "Found 2005 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "\n",
    "# Define image size and batch size\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Define paths\n",
    "TRAIN_DIR = \"Downloads/data/train\"\n",
    "VAL_DIR = \"Downloads/data/val\"\n",
    "\n",
    "# Data augmentation & normalization\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255.0,  # Normalize pixel values\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    zoom_range=0.2,   # Flip horizontally\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Load train & validation data\n",
    "train_data = datagen.flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "datagen1 = ImageDataGenerator(\n",
    "    rescale=1.0/255.0) \n",
    "\n",
    "val_data = datagen.flow_from_directory(\n",
    "    VAL_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0ed186-7eb6-4505-9eca-6346fbffa6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.MobileNetV2(\n",
    "    weights='imagenet', include_top=False, input_shape=(224, 224, 3)\n",
    ")\n",
    "\n",
    "# Freeze base model layers\n",
    "base_model.trainable = False\n",
    "\n",
    "# Add custom classification layers\n",
    "model = tf.keras.Sequential([\n",
    "    base_model,\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(516, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(train_data.num_classes, activation='softmax')  # Multi-class classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f5b20e-8783-45c3-bdcc-5538d88e7bbf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hist=model.fit(train_data,validation_data=val_data,epochs=10 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b329d27-c419-4f55-8cfb-6e41e7717a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable=True\n",
    "model.compile(  optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "hist_fine=model.fit(train_data,validation_data=val_data,epochs=5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdf560b-a28b-405e-81ac-c2b224fe3a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Enable GPU growth to avoid memory issues\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"✅ Using GPU for training\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"❌ No GPU found. Using CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b70329-aff3-49dc-90c3-2ca69aca96f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Check if GPU is available\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca79372-d807-41bc-90da-9fbb0455f165",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f739fa9-6c6e-4482-81d7-63975f084537",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow Version:\", tf.__version__)\n",
    "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
    "print(tf.test.is_built_with_cuda())  \n",
    "print(tf.test.is_built_with_gpu_support())  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72084c7-a97b-4a6a-be39-d4847f94fa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install vit-keras tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef0ebed0-3409-4380-ab5f-d5456e2f16d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\vit_keras\\utils.py:81: UserWarning: Resizing position embeddings from 24, 24 to 14, 14\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " embedding (Conv2D)          (None, 14, 14, 768)       590592    \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 196, 768)          0         \n",
      "                                                                 \n",
      " class_token (ClassToken)    (None, 197, 768)          768       \n",
      "                                                                 \n",
      " Transformer/posembed_input  (None, 197, 768)          151296    \n",
      "  (AddPositionEmbs)                                              \n",
      "                                                                 \n",
      " Transformer/encoderblock_0  ((None, 197, 768),        7087872   \n",
      "  (TransformerBlock)          (None, 12, None, None)             \n",
      "                             )                                   \n",
      "                                                                 \n",
      " Transformer/encoderblock_1  ((None, 197, 768),        7087872   \n",
      "  (TransformerBlock)          (None, 12, None, None)             \n",
      "                             )                                   \n",
      "                                                                 \n",
      " Transformer/encoderblock_2  ((None, 197, 768),        7087872   \n",
      "  (TransformerBlock)          (None, 12, None, None)             \n",
      "                             )                                   \n",
      "                                                                 \n",
      " Transformer/encoderblock_3  ((None, 197, 768),        7087872   \n",
      "  (TransformerBlock)          (None, 12, None, None)             \n",
      "                             )                                   \n",
      "                                                                 \n",
      " Transformer/encoderblock_4  ((None, 197, 768),        7087872   \n",
      "  (TransformerBlock)          (None, 12, None, None)             \n",
      "                             )                                   \n",
      "                                                                 \n",
      " Transformer/encoderblock_5  ((None, 197, 768),        7087872   \n",
      "  (TransformerBlock)          (None, 12, None, None)             \n",
      "                             )                                   \n",
      "                                                                 \n",
      " Transformer/encoderblock_6  ((None, 197, 768),        7087872   \n",
      "  (TransformerBlock)          (None, 12, None, None)             \n",
      "                             )                                   \n",
      "                                                                 \n",
      " Transformer/encoderblock_7  ((None, 197, 768),        7087872   \n",
      "  (TransformerBlock)          (None, 12, None, None)             \n",
      "                             )                                   \n",
      "                                                                 \n",
      " Transformer/encoderblock_8  ((None, 197, 768),        7087872   \n",
      "  (TransformerBlock)          (None, 12, None, None)             \n",
      "                             )                                   \n",
      "                                                                 \n",
      " Transformer/encoderblock_9  ((None, 197, 768),        7087872   \n",
      "  (TransformerBlock)          (None, 12, None, None)             \n",
      "                             )                                   \n",
      "                                                                 \n",
      " Transformer/encoderblock_1  ((None, 197, 768),        7087872   \n",
      " 0 (TransformerBlock)         (None, 12, None, None)             \n",
      "                             )                                   \n",
      "                                                                 \n",
      " Transformer/encoderblock_1  ((None, 197, 768),        7087872   \n",
      " 1 (TransformerBlock)         (None, 12, None, None)             \n",
      "                             )                                   \n",
      "                                                                 \n",
      " Transformer/encoder_norm (  (None, 197, 768)          1536      \n",
      " LayerNormalization)                                             \n",
      "                                                                 \n",
      " ExtractToken (Lambda)       (None, 768)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              787456    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 7)                 7175      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 86593287 (330.33 MB)\n",
      "Trainable params: 22059783 (84.15 MB)\n",
      "Non-trainable params: 64533504 (246.18 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from vit_keras import vit\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "num_classes = 7  # Define number of classes\n",
    "\n",
    "# Load pre-trained ViT model\n",
    "base_model = vit.vit_b16(\n",
    "    image_size=224,\n",
    "    pretrained=True,\n",
    "    include_top=False,\n",
    "    pretrained_top=False\n",
    ")\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "for layer in base_model.layers[-5:]:  \n",
    "    layer.trainable = True\n",
    "\n",
    "\n",
    "# Define custom classifier\n",
    "x = Dense(1024, activation='relu')(base_model.output)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# Create model\n",
    "model = Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d3426a-068d-4a0f-9a76-6bf4f39c3936",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow-addons\n",
    "!pip install cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdf506d-28e9-4de0-a726-6b7352c30a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d790e3d7-4620-4be8-94a6-451043843ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install opencv-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f13dda7-dc9b-4c5a-8f93-7de7244247b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history_tran = model.fit(train_data, validation_data=val_data, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49dafcf-7ae0-4ab9-aaf0-a134d77f15de",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7845c551-898b-45ab-aa83-7a4392e97f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721e13a8-a705-4d17-abee-0851b7182462",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"C:/Users/shrey/Downloads/vit_model.keras\"\n",
    "model.save(save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a49b40b2-c062-4474-9857-e552e5d33341",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = keras.models.load_model(\"C:/Users/shrey/Downloads/vit_model.keras\", safe_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c75926a0-dcb0-4b7f-968a-6c1e5f6d1e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f1ee87-5c2e-4352-93d3-1242de4ae3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1395cb-5b37-4fb0-96c3-8be2818ee83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.evaluate(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2f9fa4b-e78a-49cf-8f12-5ab1f3ea357f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "251/251 [==============================] - 9824s 39s/step - loss: 0.3652 - Accuracy: 0.8693 - val_loss: 0.4352 - val_Accuracy: 0.8489 - lr: 1.0000e-05\n",
      "Epoch 2/2\n",
      "251/251 [==============================] - 7608s 30s/step - loss: 0.3009 - Accuracy: 0.8901 - val_loss: 0.4350 - val_Accuracy: 0.8539 - lr: 1.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x19a19fbe050>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.compile(optimizer=tf.keras.optimizers.AdamW(learning_rate=1e-5),loss='categorical_crossentropy',metrics=['Accuracy'])\n",
    "model1.fit(train_data,validation_data=val_data,epochs=2,callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cc5b51e-9d77-447a-8b0a-81bb7ddf8941",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "lr_scheduler = ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\", patience=3, factor=0.5, min_lr=1e-6\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24af22dc-d861-4c6f-be0d-a43a888f1ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " embedding (Conv2D)          (None, 14, 14, 768)       590592    \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 196, 768)          0         \n",
      "                                                                 \n",
      " class_token (ClassToken)    (None, 197, 768)          768       \n",
      "                                                                 \n",
      " Transformer/posembed_input  (None, 197, 768)          151296    \n",
      "  (AddPositionEmbs)                                              \n",
      "                                                                 \n",
      " Transformer/encoderblock_0  ((None, 197, 768),        7087872   \n",
      "  (TransformerBlock)          (None, 12, None, None)             \n",
      "                             )                                   \n",
      "                                                                 \n",
      " Transformer/encoderblock_1  ((None, 197, 768),        7087872   \n",
      "  (TransformerBlock)          (None, 12, None, None)             \n",
      "                             )                                   \n",
      "                                                                 \n",
      " Transformer/encoderblock_2  ((None, 197, 768),        7087872   \n",
      "  (TransformerBlock)          (None, 12, None, None)             \n",
      "                             )                                   \n",
      "                                                                 \n",
      " Transformer/encoderblock_3  ((None, 197, 768),        7087872   \n",
      "  (TransformerBlock)          (None, 12, None, None)             \n",
      "                             )                                   \n",
      "                                                                 \n",
      " Transformer/encoderblock_4  ((None, 197, 768),        7087872   \n",
      "  (TransformerBlock)          (None, 12, None, None)             \n",
      "                             )                                   \n",
      "                                                                 \n",
      " Transformer/encoderblock_5  ((None, 197, 768),        7087872   \n",
      "  (TransformerBlock)          (None, 12, None, None)             \n",
      "                             )                                   \n",
      "                                                                 \n",
      " Transformer/encoderblock_6  ((None, 197, 768),        7087872   \n",
      "  (TransformerBlock)          (None, 12, None, None)             \n",
      "                             )                                   \n",
      "                                                                 \n",
      " Transformer/encoderblock_7  ((None, 197, 768),        7087872   \n",
      "  (TransformerBlock)          (None, 12, None, None)             \n",
      "                             )                                   \n",
      "                                                                 \n",
      " Transformer/encoderblock_8  ((None, 197, 768),        7087872   \n",
      "  (TransformerBlock)          (None, 12, None, None)             \n",
      "                             )                                   \n",
      "                                                                 \n",
      " Transformer/encoderblock_9  ((None, 197, 768),        7087872   \n",
      "  (TransformerBlock)          (None, 12, None, None)             \n",
      "                             )                                   \n",
      "                                                                 \n",
      " Transformer/encoderblock_1  ((None, 197, 768),        7087872   \n",
      " 0 (TransformerBlock)         (None, 12, None, None)             \n",
      "                             )                                   \n",
      "                                                                 \n",
      " Transformer/encoderblock_1  ((None, 197, 768),        7087872   \n",
      " 1 (TransformerBlock)         (None, 12, None, None)             \n",
      "                             )                                   \n",
      "                                                                 \n",
      " Transformer/encoder_norm (  (None, 197, 768)          1536      \n",
      " LayerNormalization)                                             \n",
      "                                                                 \n",
      " ExtractToken (Lambda)       (None, 768)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              787456    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 7)                 7175      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 86593287 (330.33 MB)\n",
      "Trainable params: 86593287 (330.33 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Unfreeze the last 10 layers for fine-tuning\n",
    "for layer in model1.layers[-15:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "model1.trainable=True\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d61d524-5725-48c6-9871-52a74d3cdfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=30,       # Rotate images by 30 degrees\n",
    "    width_shift_range=0.2,   # Shift width by 20%\n",
    "    height_shift_range=0.2,  # Shift height by 20%\n",
    "    shear_range=0.2,         # Shear intensity\n",
    "    zoom_range=0.2,          # Zoom in/out by 20%\n",
    "    horizontal_flip=True,    # Flip horizontally\n",
    "    fill_mode='nearest'      # Fill missing pixels\n",
    ")\n",
    "\n",
    "# Apply augmentation during training\n",
    "train_generator = datagen.flow(train_data, CLASS_LABELS, batch_size=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7510f6-b253-43c8-8fb7-8c7db89b7673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Convert DirectoryIterator to tf.data.Dataset\n",
    "train_data = tf.data.Dataset.from_generator(\n",
    "    lambda: train_data, \n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(224, 224, 3), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(), dtype=tf.int32)  # Adjust shape based on your labels\n",
    "    )\n",
    ")\n",
    "\n",
    "# Augment function\n",
    "def augment(image, label):\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_brightness(image, max_delta=0.2)\n",
    "    image = tf.image.random_contrast(image, 0.8, 1.2)\n",
    "    return image, label\n",
    "\n",
    "# Apply augmentation\n",
    "train_data = train_data.map(augment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df64e8ab-9d0b-4914-b447-a02cb08048bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.save(\"C:/Users/shrey/Downloads/vit_model2.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9a7c672-6784-4026-9041-98dc6f3bf8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 278s 4s/step - loss: 0.4438 - Accuracy: 0.8584\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.44381365180015564, 0.8583540916442871]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.evaluate(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ee4d22-bbeb-4a76-9b7e-e530c9ec4029",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
